commit 962f153f1ac1e7a2e31970b3de08a483f94a2866
Author: Richard Braun <rbraun@sceen.net>
Date:   Sat Dec 24 03:32:42 2016 +0100

    Fix experimental_vm_allocate_contiguous

diff --git a/vm/vm_user.c b/vm/vm_user.c
index 47d50b24..d29bbb23 100644
--- a/vm/vm_user.c
+++ b/vm/vm_user.c
@@ -487,15 +487,14 @@ kern_return_t experimental_vm_allocate_contiguous(host_priv, map, result_vaddr,
 	vm_address_t		*result_paddr;
 	vm_size_t		size;
 {
+	vm_size_t		alloc_size;
 	unsigned int		npages;
 	unsigned int		i;
 	unsigned int		order;
 	vm_page_t		pages;
 	vm_object_t		object;
-	vm_map_entry_t		entry;
 	kern_return_t		kr;
 	vm_address_t		vaddr;
-	vm_offset_t		offset = 0;
 
 	if (host_priv == HOST_NULL)
 		return KERN_INVALID_HOST;
@@ -503,82 +502,74 @@ kern_return_t experimental_vm_allocate_contiguous(host_priv, map, result_vaddr,
 	if (map == VM_MAP_NULL)
 		return KERN_INVALID_TASK;
 
+	size = vm_page_round(size);
+
+	if (size == 0)
+		return KERN_INVALID_ARGUMENT;
+
+	object = vm_object_allocate(size);
+
+	if (object == NULL)
+		return KERN_RESOURCE_SHORTAGE;
+
 	/*
 	 * XXX The page allocator returns blocks with a power-of-two size.
-	 * The requested size may not be a power-of-two, causing the pages
-	 * at the end of a block to be unused. In order to keep track of
-	 * those pages, they must all be inserted in the VM object created
-	 * by this function.
+	 * The requested size may not be a power-of-two, requiring some
+	 * work to release back the pages that aren't needed.
 	 */
 	order = vm_page_order(size);
-	size = (1 << (order + PAGE_SHIFT));
+	alloc_size = (1 << (order + PAGE_SHIFT));
+	npages = vm_page_atop(alloc_size);
 
-	/* We allocate the contiguous physical pages for the buffer. */
+	pages = vm_page_grab_contig(alloc_size, VM_PAGE_SEL_DIRECTMAP);
 
-	npages = size / PAGE_SIZE;
-	pages = vm_page_grab_contig(size, VM_PAGE_SEL_DIRECTMAP);
-	if (pages == NULL)
-	{
+	if (pages == NULL) {
+		vm_object_deallocate(object);
 		return KERN_RESOURCE_SHORTAGE;
 	}
-	
-#if 0
-	kr = vm_page_grab_contig(npages, pages, NULL, TRUE);
-	if (kr)
-	{
-		kfree (pages, npages * sizeof (vm_page_t));
-		return kr;
+
+	vm_object_lock(object);
+	vm_page_lock_queues();
+
+	for (i = 0; i < vm_page_atop(size); i++) {
+		/*
+		 * XXX We can safely handle contiguous pages as an array,
+		 * but this relies on knowing the implementation of the
+		 * page allocator.
+		 */
+		pages[i].busy = FALSE;
+		vm_page_insert(&pages[i], object, vm_page_ptoa(i));
+		vm_page_wire(&pages[i]);
 	}
-#endif
 
-	/* Allocate the object 
-	 * and find the virtual address for the DMA buffer */
+	vm_page_unlock_queues();
+	vm_object_unlock(object);
 
-	object = vm_object_allocate(size);
-	vm_map_lock(map);
-	/* TODO user_wired_count might need to be set as 1 */
-	kr = vm_map_find_entry(map, &vaddr, size, (vm_offset_t) 0,
-			       VM_OBJECT_NULL, &entry);
-	if (kr != KERN_SUCCESS) 
-	{
-		vm_map_unlock(map);
+	for (i = vm_page_atop(size); i < npages; i++) {
+		vm_page_release(&pages[i], FALSE, FALSE);
+	}
+
+	vaddr = 0;
+	kr = vm_map_enter(map, &vaddr, size, 0, TRUE, object, 0, FALSE,
+			  VM_PROT_READ | VM_PROT_WRITE,
+			  VM_PROT_READ | VM_PROT_WRITE, VM_INHERIT_DEFAULT);
+
+	if (kr != KERN_SUCCESS) {
 		vm_object_deallocate(object);
-		vm_page_free_contig(pages, size);
 		return kr;
 	}
-	        
-	entry->object.vm_object = object;
-	entry->offset = 0;
-
-	/* We can unlock map now.  */
-	vm_map_unlock(map);
 
-	/* We have physical pages we need and now we need to do the mapping. */
+	kr = vm_map_pageable(map, vaddr, vaddr + size,
+			     VM_PROT_READ | VM_PROT_WRITE,
+			     TRUE, TRUE);
 
-	pmap_pageable (map->pmap, vaddr, vaddr + size, FALSE);
+	if (kr != KERN_SUCCESS) {
+		vm_map_remove(map, vaddr, vaddr + size);
+		return kr;
+	}
 
 	*result_vaddr = vaddr;
 	*result_paddr = pages->phys_addr;
 
-	for (i = 0; i < npages; i++)
-	{
-		vm_object_lock(object);
-		vm_page_lock_queues();
-		vm_page_insert(&pages[i], object, offset);
-		vm_page_wire(&pages[i]);
-		vm_page_unlock_queues();
-		vm_object_unlock(object);
-
-		/* Enter it in the kernel pmap */
-		PMAP_ENTER(map->pmap, vaddr, &pages[i], VM_PROT_DEFAULT, TRUE);
-
-		vm_object_lock(object);
-		PAGE_WAKEUP_DONE(&pages[i]);
-		vm_object_unlock(object);
-
-		vaddr += PAGE_SIZE;
-		offset += PAGE_SIZE;
-	}
-
 	return KERN_SUCCESS;
 }
